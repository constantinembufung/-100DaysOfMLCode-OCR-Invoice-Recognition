{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with keras - sentiment analysis on IMDb dataset",
      "provenance": [],
      "authorship_tag": "ABX9TyMBF9pcEuvSPNmjX0sj3Wu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantinembufung/-100DaysOfMLCode-OCR-Invoice-Recognition/blob/master/Deep_Learning_with_keras_sentiment_analysis_on_IMDb_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V99gETRSe1L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "The IMDb dataset contains the text of 50,000 movie reviews from the internet Movie Database. Each review is either positive or negative that is thumb up or thumb down.\n",
        "we have 25,000 reviews for training and 25,000 for testing. Our goal is to build a classifier that is able to predict the binary judgment given the text\n",
        "We load the dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DicJGa7ahawB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500\n",
        "def load_data():\n",
        "        # Load data.\n",
        "        (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "        # Pad sequences with max_len.\n",
        "        X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "        X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "        return (X_train, y_train), (X_test, y_test)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voY7Q5VzjeKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    # Input: - eEmbedding Layer.\n",
        "    # The model will take as input an integer matrix of size (batch,     # input_length).\n",
        "    # The model will output dimension (input_length, dim_embedding).\n",
        "    # The largest integer in the input should be no larger\n",
        "    # than n_words (vocabulary size).\n",
        "    model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Takes the maximum value of either feature vector from each of     # the n_words features.\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKvOYLbVktG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82c9adb6-c348-46f0-e692-eb074517dc1e"
      },
      "source": [
        "#lets tain the model \n",
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "score = model.fit(X_train, y_train, epochs = EPOCHS,\n",
        "                  batch_size = BATCH_SIZE,\n",
        "                  validation_data = (X_train, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test,batch_size = BATCH_SIZE)\n",
        "print(\"\\n Test score: \" ,score[0])\n",
        "print(\"\\n Test accuracy: \" ,score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.6737 - accuracy: 0.6267 - val_loss: 0.6981 - val_accuracy: 0.4965\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.4740 - accuracy: 0.8352 - val_loss: 0.9741 - val_accuracy: 0.4990\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 23s 468ms/step - loss: 0.2870 - accuracy: 0.8839 - val_loss: 1.2369 - val_accuracy: 0.4976\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.2220 - accuracy: 0.9133 - val_loss: 1.3621 - val_accuracy: 0.4963\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 23s 468ms/step - loss: 0.1751 - accuracy: 0.9361 - val_loss: 1.5079 - val_accuracy: 0.4972\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 24s 471ms/step - loss: 0.1361 - accuracy: 0.9538 - val_loss: 1.6789 - val_accuracy: 0.4968\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 23s 468ms/step - loss: 0.1045 - accuracy: 0.9680 - val_loss: 1.8424 - val_accuracy: 0.4964\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 23s 468ms/step - loss: 0.0801 - accuracy: 0.9770 - val_loss: 1.9860 - val_accuracy: 0.4966\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 23s 466ms/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 2.1500 - val_accuracy: 0.4964\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 23s 469ms/step - loss: 0.0457 - accuracy: 0.9892 - val_loss: 2.3257 - val_accuracy: 0.4960\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 23s 469ms/step - loss: 0.0342 - accuracy: 0.9921 - val_loss: 2.4916 - val_accuracy: 0.4963\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 24s 474ms/step - loss: 0.0263 - accuracy: 0.9943 - val_loss: 2.6350 - val_accuracy: 0.4964\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.0196 - accuracy: 0.9966 - val_loss: 2.8398 - val_accuracy: 0.4964\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 25s 498ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 2.9784 - val_accuracy: 0.4964\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 24s 490ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 3.1184 - val_accuracy: 0.4963\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 24s 483ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 3.1923 - val_accuracy: 0.4964\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 24s 478ms/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 3.3311 - val_accuracy: 0.4963\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 24s 481ms/step - loss: 0.0080 - accuracy: 0.9988 - val_loss: 3.4168 - val_accuracy: 0.4963\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 24s 479ms/step - loss: 0.0073 - accuracy: 0.9989 - val_loss: 3.5377 - val_accuracy: 0.4963\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 24s 479ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 3.6291 - val_accuracy: 0.4963\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 0.4966 - accuracy: 0.8509\n",
            "\n",
            " Test score:  0.49661946296691895\n",
            "\n",
            " Test accuracy:  0.8508800268173218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoy4WGqXoeIf",
        "colab_type": "text"
      },
      "source": [
        "We attain a test accuracy of 85.09% which is a good start. "
      ]
    }
  ]
}